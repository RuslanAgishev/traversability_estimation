#!/usr/bin/env python

import cv2
import numpy as np
import torch
from argparse import ArgumentParser
from matplotlib import pyplot as plt
import datasets
import yaml
import os


# Initialize the inference transforms
def preprocessing(image, input_size, input_scale=1.0,
                  mean=np.array([0.485, 0.456, 0.406]),
                  std=np.array([0.229, 0.224, 0.225])):
    image = cv2.resize(image, input_size, fx=input_scale, fy=input_scale,
                       interpolation=cv2.INTER_AREA)
    # image shape should be divisible by 32
    h, w = image.shape[:2]
    image = cv2.resize(image, (32 * (w // 32), 32 * (h // 32)),
                       interpolation=cv2.INTER_AREA)
    image = image.astype(np.float32)
    image = image[..., ::-1]  # BGR to RGB
    image = image / 255.0

    # Re-scaled training data mean and std.
    intensity_ratio = image.mean() / mean.mean()
    mean = intensity_ratio * mean
    # std = intensity_ratio * std
    std = image.std()

    image -= mean
    image /= std

    image = image.transpose((2, 0, 1))  # HxWxC tp CxHxW
    return image


def convert_label(label, inverse=False):
    label_mapping = {0: 0,
                     1: 0,
                     3: 1,
                     4: 2,
                     5: 3,
                     6: 4,
                     7: 5,
                     8: 6,
                     9: 7,
                     10: 8,
                     12: 9,
                     15: 10,
                     17: 11,
                     18: 12,
                     19: 13,
                     23: 14,
                     27: 15,
                     #  29: 1,
                     #  30: 1,
                     31: 16,
                     #  32: 4,
                     33: 17,
                     34: 18}
    if isinstance(label, np.ndarray):
        temp = label.copy()
    elif isinstance(label, torch.Tensor):
        temp = label.clone()
    else:
        raise ValueError('Supported types: np.ndarray, torch.Tensor')
    if inverse:
        for v, k in label_mapping.items():
            temp[label == k] = v
    else:
        for k, v in label_mapping.items():
            temp[label == k] = v
    return temp


def convert_color(label, color_map):
    if isinstance(label, np.ndarray):
        temp = np.zeros(label.shape + (3,)).astype(np.uint8)
    elif isinstance(label, torch.Tensor):
        temp = torch.zeros(label.shape + (3,), dtype=torch.uint8)
    else:
        raise ValueError('Supported types: np.ndarray, torch.Tensor')
    for k, v in color_map.items():
        temp[label == k] = v
    return temp


parser = ArgumentParser()
parser.add_argument('--dataset', type=str, default='Rellis3D')
parser.add_argument('--img_size', nargs='+', default=(704, 960))
parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
args = parser.parse_args()
args.img_size = tuple(args.img_size)

Dataset = eval('datasets.%s' % args.dataset)
ds = Dataset(crop_size=args.img_size, split='test')

# Initialize model with the best available weights
model = torch.load("fcn_resnet50_legacy.pth").to(args.device)
model.eval()

# Apply inference preprocessing transforms
img, _ = ds[0]
size = (960, 704)
img = preprocessing(img, input_size=size)
batch = torch.from_numpy(img).unsqueeze(0).to(args.device)

# Use the model and visualize the prediction
with torch.no_grad():
    pred = model(batch)["out"]
pred = pred.softmax(dim=1)
pred = pred.squeeze(0).cpu().numpy()
mask = np.argmax(pred, axis=0)
mask = convert_label(mask, inverse=True)
mask = cv2.resize(mask.astype('float32'), size, interpolation=cv2.INTER_LINEAR).astype('int8')

pkg_path = os.path.realpath(os.path.join(os.path.dirname(__file__), '../../'))
label_config = os.path.join(pkg_path, "config/rellis.yaml")
data_cfg = yaml.safe_load(open(label_config, 'r'))

result = convert_color(mask, data_cfg['color_map'])
plt.imshow(result)
plt.show()
