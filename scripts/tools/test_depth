#!/usr/bin/env python

import numpy as np
import torch
from argparse import ArgumentParser
import datasets
import os
from traversability_estimation.utils import visualize_imgs


def main():
    parser = ArgumentParser()
    # parser.add_argument('--dataset', type=str, default='Rellis3DClouds')
    # parser.add_argument('--dataset', type=str, default='TraversabilityClouds')
    parser.add_argument('--dataset', type=str, default='FlexibilityClouds')
    parser.add_argument('--weights', type=str, default='deeplabv3_resnet101_lr_0.0001_bs_8_epoch_66_FlexibilityClouds_depth_labels_flexibility_iou_0.537.pth')
    parser.add_argument('--device', type=str, default='cpu')
    args = parser.parse_args()
    print(args)

    pkg_path = os.path.realpath(os.path.join(os.path.dirname(__file__), '../../'))

    # Initialize model with the best available weights
    model_name = args.weights
    assert args.dataset in model_name
    model = torch.load(os.path.join(pkg_path, 'config/weights/depth_cloud', model_name), map_location=args.device)
    # model = torch.load(model_name, map_location=args.device)
    model.eval()

    data_fields = [f[1:-1] for f in ['_x_', '_y_', '_z_', '_intensity_', '_depth_'] if f in model_name]
    print('Model takes as input: %s' % ','.join(data_fields))

    if 'Traversability' in model_name:
        labels_mapping = 'traversability'
    elif 'Flexibility' in model_name:
        labels_mapping = 'flexibility'
    else:
        labels_mapping = None

    Dataset = eval('datasets.%s' % args.dataset)
    ds = Dataset(split='test', fields=data_fields,
                 labels_mapping=labels_mapping,
                 lidar_beams_step=2 if 'Rellis' in args.dataset else 1)

    for _ in range(5):
        # Apply inference preprocessing transforms
        inpt, label = ds[np.random.choice(range(len(ds)))]

        depth_img = inpt[0]
        power = 16
        depth_img_vis = np.copy(depth_img).squeeze()  # depth
        depth_img_vis[depth_img_vis > 0] = depth_img_vis[depth_img_vis > 0] ** (1 / power)
        depth_img_vis[depth_img_vis > 0] = (depth_img_vis[depth_img_vis > 0] - depth_img_vis[depth_img_vis > 0].min()) / \
                                           (depth_img_vis[depth_img_vis > 0].max() - depth_img_vis[
                                               depth_img_vis > 0].min())

        # Use the model and visualize the prediction
        batch = torch.from_numpy(inpt).unsqueeze(0).to(args.device)
        with torch.no_grad():
            pred = model(batch)['out']
        pred = torch.softmax(pred.squeeze(0), dim=0).cpu().numpy()
        pred = np.argmax(pred, axis=0)

        color_pred = ds.label_to_color(pred)
        color_gt = ds.label_to_color(label)

        visualize_imgs(layout='columns',
                       depth_img=depth_img_vis,
                       prediction=color_pred,
                       ground_truth=color_gt,
                       )


if __name__ == '__main__':
    main()
