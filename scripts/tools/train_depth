#!/usr/bin/env python

import os
import numpy as np
import torchvision.models.segmentation
import torch
from torch.utils.data import DataLoader, BatchSampler
from torch.utils.data.sampler import SequentialSampler
from torch.utils.tensorboard import SummaryWriter
from torch.nn import functional as F
from argparse import ArgumentParser
import datasets
from tqdm import tqdm
from time import time
import segmentation_models_pytorch as smp
from typing import Iterator, List
from traversability_estimation.utils import visualize
from datasets.base_dataset import VOID_VALUE


def parse_arguments():
    parser = ArgumentParser()
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--datasets', nargs='+', type=str, default='Rellis3DClouds')
    parser.add_argument('--labels_mapping', type=str, default=None)
    parser.add_argument('--dont_save_models', action='store_true')
    parser.add_argument('--architecture', type=str, default='fcn_resnet101')
    parser.add_argument('--batch_size', type=int, default=2)
    parser.add_argument('--n_epochs', type=int, default=100)
    parser.add_argument('--n_workers', type=int, default=os.cpu_count() // 2)
    parser.add_argument('--data_fields', nargs='+', type=str, default=None)
    parser.add_argument('--n_samples', type=int, default=None)
    parser.add_argument('--vis_preds', action='store_true')
    args = parser.parse_args()

    return args


def create_model(architecture, n_inputs, n_outputs, pretrained_backbone=True):
    assert architecture in ['fcn_resnet50', 'fcn_resnet101', 'deeplabv3_resnet50', 'deeplabv3_resnet101',
                            'deeplabv3_mobilenet_v3_large', 'lraspp_mobilenet_v3_large']

    print('Creating model %s with %i inputs and %i outputs' % (architecture, n_inputs, n_outputs))
    Architecture = eval('torchvision.models.segmentation.%s' % architecture)
    model = Architecture(pretrained=pretrained_backbone)

    arch = architecture.split('_')[0]
    encoder = '_'.join(architecture.split('_')[1:])

    # Change input layer to accept n_inputs
    if encoder == 'mobilenet_v3_large':
        model.backbone['0'][0] = torch.nn.Conv2d(n_inputs, 16,
                                                 kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    else:
        model.backbone['conv1'] = torch.nn.Conv2d(n_inputs, 64,
                                                  kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)

    # Change final layer to output n classes
    if arch == 'lraspp':
        model.classifier.low_classifier = torch.nn.Conv2d(40, n_outputs, kernel_size=(1, 1), stride=(1, 1))
        model.classifier.high_classifier = torch.nn.Conv2d(128, n_outputs, kernel_size=(1, 1), stride=(1, 1))
    elif arch == 'fcn':
        model.classifier[-1] = torch.nn.Conv2d(512, n_outputs, kernel_size=(1, 1), stride=(1, 1))
    elif arch == 'deeplabv3':
        model.classifier[-1] = torch.nn.Conv2d(256, n_outputs, kernel_size=(1, 1), stride=(1, 1))

    return model


class CommonBatchSampler(BatchSampler):
    """
    Selecting indices from two different datasets to form batches of data
    """

    def __init__(self, sampler, batch_size, drop_last, shuffle=False):
        super(CommonBatchSampler, self).__init__(sampler=sampler, batch_size=batch_size, drop_last=drop_last)
        self.shuffle = shuffle

    def __iter__(self) -> Iterator[List[int]]:
        datasets_idx_border = self.sampler.data_source.cumulative_sizes[0]

        # assume we have 2 datasets two concatenate
        assert len(self.sampler.data_source.cumulative_sizes) == 2
        if self.shuffle:
            # random batch sampler: select batches randomly either from one dataset or another
            ids = {'0': list(self.sampler)[:datasets_idx_border],
                   '1': list(self.sampler)[datasets_idx_border:]}
            while len(ids['0']) > 0 or len(ids['1']) > 0:
                data_id = np.random.choice(list(ids.keys()))
                if len(ids[data_id]) == 0:
                    # inverse 0 to 1 and vise versa
                    data_id = str(int(not int(data_id)))
                assert len(ids[data_id]) > 0

                if len(ids[data_id]) > self.batch_size:
                    batch = np.random.choice(ids[data_id], self.batch_size, replace=False).tolist()
                    assert len(batch) > 0
                else:
                    if self.drop_last:
                        break
                    else:
                        batch = ids[data_id].copy()
                        assert len(batch) > 0

                for ind in batch:
                    ids[data_id].remove(ind)
                assert len(batch) > 0
                yield batch
        else:
            # sequential batch sampler
            batch = []
            for idx in self.sampler:
                batch.append(idx)
                if len(batch) == self.batch_size or idx == datasets_idx_border - 1:
                    yield batch
                    batch = []
            if len(batch) > 0 and not self.drop_last:
                yield batch


def create_dataloaders(args):
    print('Using datasets for training: %s' % ' '.join(args.datasets))

    Dataset = eval('datasets.%s' % args.datasets[0])
    # lidar_beams_step = 2 in order to have horizontal resolution = 1024 (instead of 2048 as in Rellis data)
    train_dataset = Dataset(split='train', lidar_beams_step=2 if 'Rellis' in args.datasets[0] else None,
                            labels_mapping=args.labels_mapping,
                            fields=args.data_fields, num_samples=args.n_samples,
                            labels_mode='labels')
    valid_dataset = Dataset(split='val', lidar_beams_step=2 if 'Rellis' in args.datasets[0] else None,
                            labels_mapping=args.labels_mapping,
                            fields=args.data_fields, num_samples=args.n_samples,
                            labels_mode='labels')

    train_datasets, valid_datasets = [train_dataset], [valid_dataset]

    if len(args.datasets) > 1:
        assert len(args.datasets) == 2
        # fine tuning is only supported for traversability or flexibility labels
        assert train_dataset.labels_mapping in ['traversability', 'flexibility']

        DatasetFT = eval('datasets.%s' % args.datasets[1])

        ft_dataset_train = DatasetFT(split='train',
                                     labels_mapping=args.labels_mapping,
                                     fields=args.data_fields, num_samples=args.n_samples,
                                     labels_mode='labels')
        assert ft_dataset_train.labels_mapping == train_dataset.labels_mapping

        train_dataset_combined = torch.utils.data.ConcatDataset([train_dataset, ft_dataset_train])

        # https://stackoverflow.com/questions/51837110/pytorch-data-loading-from-multiple-different-sized-datasets
        batch_sampler = CommonBatchSampler(SequentialSampler(train_dataset_combined),
                                           batch_size=args.batch_size,
                                           drop_last=False,
                                           shuffle=True)
        train_loader = DataLoader(dataset=train_dataset_combined,
                                  num_workers=args.n_workers,
                                  batch_sampler=batch_sampler,
                                  pin_memory=True)

        ft_dataset_val = DatasetFT(split='val',
                                   labels_mapping=args.labels_mapping,
                                   fields=args.data_fields, num_samples=args.n_samples,
                                   labels_mode='labels')
        assert ft_dataset_val.labels_mapping == valid_dataset.labels_mapping

        valid_dataset_combined = torch.utils.data.ConcatDataset([valid_dataset, ft_dataset_val])

        # https://stackoverflow.com/questions/51837110/pytorch-data-loading-from-multiple-different-sized-datasets
        batch_sampler_val = CommonBatchSampler(SequentialSampler(valid_dataset_combined),
                                               batch_size=args.batch_size,
                                               drop_last=False,
                                               shuffle=False)
        valid_loader = DataLoader(dataset=valid_dataset_combined,
                                  num_workers=args.n_workers,
                                  batch_sampler=batch_sampler_val,
                                  pin_memory=True)

        train_datasets.append(ft_dataset_train)
        valid_datasets.append(ft_dataset_val)
    else:
        assert len(args.datasets) == 1

        train_loader = DataLoader(train_dataset,
                                  batch_size=args.batch_size,
                                  shuffle=True,
                                  num_workers=args.n_workers,
                                  pin_memory=True)

        valid_loader = DataLoader(valid_dataset,
                                  batch_size=1,
                                  shuffle=False,
                                  num_workers=args.n_workers,
                                  pin_memory=True)

    return train_datasets, valid_datasets, train_loader, valid_loader


# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
# background_channel = None
iou_fn = smp.utils.metrics.IoU(threshold=0.5)


def metric_fn(pred, labels, ignore_label=VOID_VALUE):
    N, C, H, W = pred.shape

    assert labels.shape == (N, H, W)
    labels_one_hot = labels.clone()

    mask = labels_one_hot != ignore_label

    pred = torch.softmax(pred, dim=1)
    pred = pred * mask.unsqueeze(1)

    labels_one_hot = F.one_hot((labels_one_hot * mask).to(torch.long).view(N, H * W), C)  # N,H*W -> N,H*W, C
    labels_one_hot = labels_one_hot.permute(0, 2, 1)  # N, C, H*W
    labels_one_hot = labels_one_hot.reshape((N, C, H, W))

    assert pred.shape == labels_one_hot.shape == (N, C, H, W)
    iou = iou_fn.forward(y_pr=pred, y_gt=labels_one_hot)

    return iou


class Trainer(object):

    def __init__(self, args):
        self.train_datasets, \
        self.valid_datasets, \
        self.train_loader, \
        self.valid_loader = create_dataloaders(args)

        self.cfg = args

        # --------------Load and set model and optimizer-------------------------------------
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        # self.device = torch.device('cpu')

        self.model = self.prepare_model(args)
        # Create adam optimizer
        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=args.lr)

        # Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
        ignore_label = 0 if not VOID_VALUE in self.train_datasets[0].class_values else VOID_VALUE
        print('Ignoring label value: %i' % ignore_label)
        self.criterion_fn = smp.losses.DiceLoss(mode='multiclass',
                                                log_loss=True,
                                                from_logits=True,
                                                ignore_index=ignore_label)

        log_dir = '%s_lr_%g_bs_%d_%s_%s_labels_%s_%s' % \
                  (args.architecture, args.lr, args.batch_size,
                   '_'.join(args.datasets), '_'.join(self.train_datasets[0].fields),
                   self.train_datasets[0].labels_mapping, time())
        self.tb_logger = SummaryWriter(log_dir=os.path.join(os.path.dirname(__file__), 'tb_runs', log_dir))

    def __str__(self):
        return 'Training a model: %s\n' \
               'with batch size: %s\n' \
               'using input: %s\n' \
               'on datasets: %s\n' \
               'Rellis labels mapping mode: %s\n' \
               'initial learning rate: %s' % \
               (self.cfg.architecture, self.cfg.batch_size, ' '.join(self.cfg.data_fields), ' '.join(self.cfg.datasets),\
                self.cfg.labels_mapping, self.cfg.lr)

    def prepare_model(self, args):
        n_inputs = self.train_datasets[0][0][0].shape[0]
        n_classes = len(self.train_datasets[0].class_values)
        print('Model takes as input %i argument: %s' % (n_inputs, str(args.data_fields)))

        model = create_model(args.architecture, n_inputs, n_classes, pretrained_backbone=False)
        model = model.to(self.device)
        return model

    def train_epoch(self):
        losses_epoch = []
        for itr, sample in tqdm(enumerate(self.train_loader)):
            inpt, labels = sample
            inpt, labels = inpt.to(self.device), labels.to(self.device)

            pred = self.model(inpt)['out']  # make prediction

            self.optimizer.zero_grad()
            loss = self.criterion_fn(pred, labels.long())  # Calculate loss
            loss.backward()  # Backpropagate loss
            self.optimizer.step()  # Apply gradient descent change to weight

            losses_epoch.append(loss.item())
            self.tb_logger.add_scalar('Train_Dice_Loss(iter) during one epoch', loss.item(), itr)

        return np.mean(losses_epoch)

    def val_epoch(self):
        # validation epoch
        metrics_epoch = []
        losses_epoch = []
        for itr, sample in tqdm(enumerate(self.valid_loader)):
            inpt, labels = sample
            inpt, labels = inpt.to(self.device), labels.to(self.device)

            with torch.no_grad():
                pred = self.model(inpt)['out']  # make prediction

                metric_sample = metric_fn(pred, labels)
                loss_val = self.criterion_fn(pred, labels.long())

            iou = metric_sample.cpu().numpy()
            metrics_epoch.append(iou)
            losses_epoch.append(loss_val.item())

            self.tb_logger.add_scalar('Valid_mIoU(iter) during one epoch', iou, itr)
            self.tb_logger.add_scalar('Valid_Dice_Loss(iter) during one epoch', loss_val.item(), itr)

        metric_val = np.mean(metrics_epoch)
        loss_val = np.mean(losses_epoch)

        return loss_val, metric_val

    def test_model(self, dataset=None):
        # Use the current trained model and visualize a prediction
        if dataset is None:
            dataset = self.valid_datasets[-1]
        inpt, label = dataset[np.random.choice(range(len(dataset)))]

        inpt = torch.from_numpy(inpt[None]).to(self.device)
        label = torch.from_numpy(label[None]).to(self.device)

        with torch.no_grad():
            pred = self.model(inpt)['out']

        pred = pred.squeeze(0).cpu().numpy()
        label = label.squeeze(0).cpu().numpy()

        color_pred = self.valid_datasets[0].label_to_color(pred)
        color_gt = self.valid_datasets[0].label_to_color(label)

        power = 16
        depth_img = np.copy(inpt.squeeze(0).cpu().numpy()[-1])  # depth
        depth_img[depth_img > 0] = depth_img[depth_img > 0] ** (1 / power)
        depth_img[depth_img > 0] = (depth_img[depth_img > 0] - depth_img[depth_img > 0].min()) / \
                                   (depth_img[depth_img > 0].max() - depth_img[depth_img > 0].min())

        visualize(layout='columns',
                  prediction=color_pred,
                  ground_truth=color_gt,
                  depth_img=depth_img,
                  )

    def train(self):
        print(self)

        max_metric = -np.Inf

        for epoch_n in tqdm(range(self.cfg.n_epochs)):
            print('Starting training epoch %i...' % epoch_n)
            # train epoch
            self.model = self.model.train()
            loss_train = self.train_epoch()

            print('Train loss at epoch %i: %f' % (epoch_n, float(loss_train)))
            self.tb_logger.add_scalar('Train_Dice_Loss(epoch)', loss_train, epoch_n)

            print('Validation ...')
            self.model = self.model.eval()
            loss_val, metric_val = self.val_epoch()

            if not self.cfg.dont_save_models:
                # save better model
                if max_metric < metric_val:  # Save model weights
                    max_metric = metric_val

                if max_metric <= metric_val or epoch_n % 10 == 0:

                    name = '%s_lr_%g_bs_%d_epoch_%d_%s_%s_labels_%s_iou_%.2f.pth' % \
                           (self.cfg.architecture,
                            self.cfg.lr, self.cfg.batch_size, epoch_n,
                            '_'.join(self.cfg.datasets), '_'.join(self.train_datasets[0].fields),
                            self.train_datasets[0].labels_mapping, float(metric_val))

                    print("Saving Model:", name)
                    torch.save(self.model, os.path.join(os.path.dirname(__file__), name))

            print('Validation mIoU at epoch %i: %f' % (epoch_n, float(metric_val)))
            self.tb_logger.add_scalar('Valid_mIoU(epoch)', metric_val, epoch_n)
            self.tb_logger.add_scalar('Valid_Dice_Loss(epoch)', loss_val, epoch_n)

            # change learning rate
            if epoch_n == 60:
                self.optimizer.param_groups[0]['lr'] /= 10.0
                print('Decrease decoder learning rate to %f !' % self.optimizer.param_groups[0]['lr'])

            if self.cfg.vis_preds:
                self.model = self.model.eval()
                self.test_model()

            self.tb_logger.close()


def main():
    args = parse_arguments()
    trainer = Trainer(args)
    trainer.train()


if __name__ == '__main__':
    main()
